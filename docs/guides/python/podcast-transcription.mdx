---
description: Use the Nitric framework to build a service for creating podcast transcripts.
tags:
  - API
  - AI & Machine Learning
languages:
  - python
published_at: 2024-11-07
updated_at: 2024-11-07
---

# Transcribing Podcasts using OpenAI Whisper

## Prerequisites

- [uv](https://docs.astral.sh/uv/#getting-started) - for Python dependency management
- The [Nitric CLI](/get-started/installation)
- _(optional)_ An [AWS](https://aws.amazon.com) account

## Getting started

We'll start by creating a new project using Nitric's python starter template.

<Note>
  If you want to take a look at the finished code, it can be found
  [here](https://github.com/nitrictech/examples/tree/main/v1/podcast-transcription).
</Note>

```bash
nitric new podcast-transcription py-starter
cd podcast-transcription
```

Next, let's install our base dependencies, then add the `openai-whisper` library as an optional dependency.

```bash
# Install the base dependencies
uv sync
# Add OpenAI whisper dependency
uv add openai-whisper librosa numpy --optional ml
```

<Note>
  We add the extra dependencies to the 'ml' optional dependencies to keep them
  separate since they can be quite large. This lets us just install them in the
  containers that need them.
</Note>

We'll organize our project structure like so:

```text
+--common/
|  +-- __init__.py
|  +-- resources.py
+--batches/
|  +-- transcribe.py
+--services/
|  +-- api.py
+--.gitignore
+--.python-version
+-- pyproject.toml
+-- python.dockerfile
+-- python.dockerignore
+-- nitric.yaml
+-- transcribe.dockerfile
+-- transcribe.dockerignore
+-- README.md
```

## Define our resources

We'll start by creating a file to define our Nitric resources. For this project we'll need an API, Batch Job, and two buckets, one for the audio files to be transcribed and one for the resulting transcripts. The API will interface with the buckets, while the Batch Job will handle the transcription.

```python title:common/resources.py
from nitric.resources import job, bucket, api

main_api = api("main")

transcribe_job = job("transcribe")

podcast_bucket = bucket("podcasts")
transcript_bucket = bucket("transcripts")
```

## Add our transcription

Now that we have defined resources, we can import our API and add some routes to access the buckets. Start by importing the resources and adding permissions to the resources.

```python title:services/api.py
from common.resources import main_api, transcript_bucket, podcast_bucket, transcribe_job
from nitric.application import Nitric
from nitric.resources import BucketNotificationContext
from nitric.context import HttpContext

readable_transcript_bucket = transcript_bucket.allow("read")
submittable_transcribe_job = transcribe_job.allow("submit")

Nitric.run()
```

We'll then write a route for getting a file from the transcription bucket. These will get a signed download url and redirect the user to this url for downloading the text content.

```python title:services/api.py
# !collapse(1:7) collapsed
from common.resources import main_api, transcript_bucket, podcast_bucket, transcribe_job
from nitric.application import Nitric
from nitric.resources import BucketNotificationContext
from nitric.context import HttpContext

readable_transcript_bucket = transcript_bucket.allow("read")
submittable_transcribe_job = transcribe_job.allow("submit")

@main_api.get("/podcast/:name")
async def get_podcast(ctx: HttpContext):
  name = ctx.req.params['name']

  download_url = await readable_transcript_bucket.file(f"{name}-transcript.txt").download_url()

  ctx.res.headers["Location"] = download_url
  ctx.res.status = 303

  return ctx

Nitric.run()
```

We will then add a route to get an upload URL for the bucket. We will do this via a URL as it circumvents the size limits of requests to the API Gateway.

```python title:services/api.py
# !collapse(1:18) collapsed
from common.resources import main_api, transcript_bucket, podcast_bucket, transcribe_job
from nitric.application import Nitric
from nitric.resources import BucketNotificationContext
from nitric.context import HttpContext

readable_transcript_bucket = transcript_bucket.allow("read")
submittable_transcribe_job = transcribe_job.allow("submit")

@main_api.get("/podcast/:name")
async def get_podcast(ctx: HttpContext):
  name = ctx.req.params['name']

  download_url = await readable_transcript_bucket.file(f"{name}-transcript.txt").download_url()

  ctx.res.headers["Location"] = download_url
  ctx.res.status = 303

  return ctx

@main_api.get("/audio-upload-url/:name")
async def get_audio_upload_url(ctx: HttpContext):
  name = ctx.req.params['name']

  upload_url = await writable_podcast_bucket.file(name).upload_url()

  ctx.res.body = upload_url

Nitric.run()
```

We will add a storage listener which will be triggered by files being added to the `podcast_bucket`.

```python title:services/api.py
# !collapse(1:26) collapsed
from common.resources import main_api, transcript_bucket, podcast_bucket, transcribe_job
from nitric.application import Nitric
from nitric.resources import BucketNotificationContext
from nitric.context import HttpContext

readable_transcript_bucket = transcript_bucket.allow("read")
submittable_transcribe_job = transcribe_job.allow("submit")

@main_api.get("/transcript/:name")
async def get_podcast(ctx: HttpContext):
  name = ctx.req.params['name']

  download_url = await readable_transcript_bucket.file(f"{name}-transcript.txt").download_url()

  ctx.res.headers["Location"] = download_url
  ctx.res.status = 303

  return ctx

@main_api.get("/podcast/:name")
async def get_podcast(ctx: HttpContext):
  name = ctx.req.params['name']

  download_url = await readable_transcript_bucket.file(f"{name}-transcript.txt").download_url()

  ctx.res.headers["Location"] = download_url
  ctx.res.status = 303

  return ctx

@podcast_bucket.on("write", "*")
async def on_add_podcast(ctx: BucketNotificationContext):
  await submittable_transcribe_job.submit({ "podcast_name": ctx.req.key })

  return ctx

Nitric.run()
```

## Downloading our model

We can download our model and embed it into our container to reduce the start up time of our transcription. We'll create a script which can be triggered using `uv run download_model.py --model_name turbo`.

```python title:download_model.py
from whisper import _MODELS, _download
import argparse
import os

default = os.path.join(os.path.expanduser("~"), ".cache")
download_root = os.path.join(os.getenv("XDG_CACHE_HOME", default), "whisper")

def download_whisper_model(model_name="base"):
  print("downloading model...")
  # if we have the original download go to the default whisper cache
  model = _download(_MODELS[model_name], root=download_root, in_memory=True)

  # make sure the ./model directory exists
  os.makedirs("./.model", exist_ok=True)

  # write the model to disk
  save_path = f"./.model/model.pt"
  with open(save_path, "wb") as f:
    f.write(model)

  print(f"Model '{model_name}' has been downloaded and saved to './model/model.pt'.")

if __name__ == "__main__":
  parser = argparse.ArgumentParser(description="Download a Whisper model.")
  parser.add_argument("--model_name", type=str, default="base", help="Name of the model to download.")

  args = parser.parse_args()

  download_whisper_model(model_name=args.model_name)
```

## Add Transcribe Batch Job

Start by adding our imports and adding permissions to the resources we defined earlier. We'll also get the location of the model set as an environment variable, defaulting to `./.model/model.pt`.

```python title:batches/transcribe.py
import whisper
import io
import numpy as np
import os
import librosa
from common.resources import transcribe_job, transcript_bucket, podcast_bucket
from nitric.context import JobContext
from nitric.application import Nitric

writeable_transcript_bucket = transcript_bucket.allow("write")
readable_podcast_bucket = podcast_bucket.allow("read")

MODEL = os.environ.get("MODEL", "./.model/model.pt")

Nitric.run()
```

We'll then create our Job and set the required memory to `12000`. This is a safe amount if you're using anything less than the large model. You can see the memory requirements for each of the model sizes below.

| Size   | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |
| ------ | ---------- | ------------------ | ------------------ | ------------- | -------------- |
| tiny   | 39 M       | tiny.en            | tiny               | `~1 GB `      | `~32x `        |
| base   | 74 M       | base.en            | base               | `~1 GB`       | `~16x`         |
| small  | 244 M      | small.en           | small              | `~2 GB`       | `~6x`          |
| medium | 769 M      | medium.en          | medium             | `~5 GB`       | `~2x`          |
| large  | 1550 M     | N/A                | large              | `~10 GB`      | `1x`           |

```python title:batches/transcribe.py
# !collapse(1:13) collapsed
import whisper
import io
import numpy as np
import os
import librosa
from common.resources import transcribe_job, transcript_bucket, podcast_bucket
from nitric.context import JobContext
from nitric.application import Nitric

writeable_transcript_bucket = transcript_bucket.allow("write")
readable_podcast_bucket = podcast_bucket.allow("read")

MODEL = os.environ.get("MODEL", "./.model/model.pt")

@transcribe_job(cpus=1, memory=12000, gpus=0)
async def transcribe_podcast(ctx: JobContext):
  return ctx

Nitric.run()
```

We'll then read the audio file that is referenced in the `JobContext` data that was sent with the submit request. We'll load these bytes into a variable as a floating point time series using `librosa` so that it can be converted to a `numpy` array for use by the `whisper`.

```python title:batches/transcribe.py
# !collapse(1:13) collapsed
import whisper
import io
import numpy as np
import os
import librosa
from common.resources import transcribe_job, transcript_bucket, podcast_bucket
from nitric.context import JobContext
from nitric.application import Nitric

writeable_transcript_bucket = transcript_bucket.allow("write")
readable_podcast_bucket = podcast_bucket.allow("read")

MODEL = os.environ.get("MODEL", "./.model/model.pt")

@transcribe_job(cpus=1, memory=12000, gpus=0)
async def transcribe_podcast(ctx: JobContext):
  podcast_name = ctx.req.data["podcast_name"]
  print(f"Transcribing: {podcast_name}")

  podcast = await readable_podcast_bucket.file(podcast_name).read()

  podcast_io = io.BytesIO(podcast)

  y, sr = librosa.load(podcast_io)
  audio_array = np.array(y)

  return ctx

Nitric.run()
```

We'll then load our model and transcribe the audio. We can turn off `FP16` with `fp16=False` which will use `FP32` instead. This will depend on what is supported on your CPU when testing locally, however, `FP16` and `FP32` are supported on Lambda.

```python title:batches/transcribe.py
# !collapse(1:13) collapsed
import whisper
import io
import numpy as np
import os
import librosa
from common.resources import transcribe_job, transcript_bucket, podcast_bucket
from nitric.context import JobContext
from nitric.application import Nitric

writeable_transcript_bucket = transcript_bucket.allow("write")
readable_podcast_bucket = podcast_bucket.allow("read")

MODEL = os.environ.get("MODEL", "./.model/model.pt")

@transcribe_job(cpus=1, memory=12000, gpus=0)
# !collapse(1:10) collapsed
async def transcribe_podcast(ctx: JobContext):
  podcast_name = ctx.req.data["podcast_name"]
  print(f"Transcribing: {podcast_name}")

  podcast = await readable_podcast_bucket.file(podcast_name).read()

  podcast_io = io.BytesIO(podcast)

  y, sr = librosa.load(podcast_io)
  audio_array = np.array(y)

  model = whisper.load_model(MODEL)
  result = model.transcribe(audio_array, verbose=True, fp16=False)

  return ctx

Nitric.run()
```

Finally, we'll take the outputted transcript and write that to the transcript bucket. This transcript is stored in `result["text"]`.

```python title:batches/transcribe.py
# !collapse(1:13) collapsed
import whisper
import io
import numpy as np
import os
import librosa
from common.resources import transcribe_job, transcript_bucket, podcast_bucket
from nitric.context import JobContext
from nitric.application import Nitric

writeable_transcript_bucket = transcript_bucket.allow("write")
readable_podcast_bucket = podcast_bucket.allow("read")

MODEL = os.environ.get("MODEL", "./.model/model.pt")

@transcribe_job(cpus=1, memory=12000, gpus=0)
# !collapse(1:13) collapsed
async def transcribe_podcast(ctx: JobContext):
  podcast_name = ctx.req.data["podcast_name"]
  print(f"Transcribing: {podcast_name}")

  podcast = await readable_podcast_bucket.file(podcast_name).read()

  podcast_io = io.BytesIO(podcast)

  y, sr = librosa.load(podcast_io)
  audio_array = np.array(y)

  model = whisper.load_model(MODEL)
  result = model.transcribe(audio_array, verbose=True, fp16=False)

  transcript = result["text"].encode()

  print("Finished transcoding... Writing to Bucket")
  await writeable_transcript_bucket.file(f"{podcast_name}-transcript.txt").write(transcript)
  print("Done!")

  return ctx

Nitric.run()
```

## Deployment Dockerfiles

With our code complete, we can write a dockerfile that our batch job will run in. Start with the base image that copies our application code and resolves the dependencies using `uv`.

```docker title:transcribe.dockerfile
# The python version must match the version in .python-version
FROM ghcr.io/astral-sh/uv:python3.11-bookworm-slim AS builder

ARG HANDLER
ENV HANDLER=${HANDLER}

ENV UV_COMPILE_BYTECODE=1 UV_LINK_MODE=copy PYTHONPATH=.
WORKDIR /app
RUN --mount=type=cache,target=/root/.cache/uv \
  --mount=type=bind,source=uv.lock,target=uv.lock \
  --mount=type=bind,source=pyproject.toml,target=pyproject.toml \
  uv sync --frozen -v --no-install-project --extra ml --no-dev --no-python-downloads
COPY . /app
RUN --mount=type=cache,target=/root/.cache/uv \
  uv sync --frozen -v --no-dev --extra ml --no-python-downloads
```

The next stage is to build upon our base with another image with Nvidia drivers. We'll set some environment variables to enable GPU use and download Python 3.11 with apt.

```docker title:transcribe.dockerfile
# !collapse(1:14) collapsed
FROM ghcr.io/astral-sh/uv:python3.11-bookworm-slim AS builder

ARG HANDLER
ENV HANDLER=${HANDLER}

ENV UV_COMPILE_BYTECODE=1 UV_LINK_MODE=copy PYTHONPATH=.
WORKDIR /app
RUN --mount=type=cache,target=/root/.cache/uv \
  --mount=type=bind,source=uv.lock,target=uv.lock \
  --mount=type=bind,source=pyproject.toml,target=pyproject.toml \
  uv sync --frozen -v --no-install-project --extra ml --no-dev --no-python-downloads
COPY . /app
RUN --mount=type=cache,target=/root/.cache/uv \
  uv sync --frozen -v --no-dev --extra ml --no-python-downloads

FROM nvcr.io/nvidia/driver:550-5.15.0-1065-nvidia-ubuntu22.04

ARG HANDLER

ENV HANDLER=${HANDLER}
ENV PYTHONUNBUFFERED=TRUE
ENV PYTHONPATH="."
ENV NVIDIA_DRIVER_CAPABILITIES=all
ENV NVIDIA_REQUIRE_CUDA="cuda>=8.0"

RUN apt-get update -y && \
  apt-get install -y software-properties-common ffmpeg && \
  add-apt-repository ppa:deadsnakes/ppa && \
  apt-get update -y && \
  apt-get install -y python3.11 && \
  ln -sf /usr/bin/python3.11 /usr/local/bin/python3.11
```

Finally, we'll get our application from the base image and run our application.

```docker title:transcribe.dockerfile
# !collapse(1:31) collapsed
FROM ghcr.io/astral-sh/uv:python3.11-bookworm-slim AS builder

ARG HANDLER
ENV HANDLER=${HANDLER}

ENV UV_COMPILE_BYTECODE=1 UV_LINK_MODE=copy PYTHONPATH=.
WORKDIR /app
RUN --mount=type=cache,target=/root/.cache/uv \
  --mount=type=bind,source=uv.lock,target=uv.lock \
  --mount=type=bind,source=pyproject.toml,target=pyproject.toml \
  uv sync --frozen -v --no-install-project --extra ml --no-dev --no-python-downloads
COPY . /app
RUN --mount=type=cache,target=/root/.cache/uv \
  uv sync --frozen -v --no-dev --extra ml --no-python-downloads

FROM nvcr.io/nvidia/driver:550-5.15.0-1065-nvidia-ubuntu22.04

ARG HANDLER

ENV HANDLER=${HANDLER}
ENV PYTHONUNBUFFERED=TRUE
ENV PYTHONPATH="."
ENV NVIDIA_DRIVER_CAPABILITIES=all
ENV NVIDIA_REQUIRE_CUDA="cuda>=8.0"

RUN apt-get update -y && \
  apt-get install -y software-properties-common ffmpeg && \
  add-apt-repository ppa:deadsnakes/ppa && \
  apt-get update -y && \
  apt-get install -y python3.11 && \
  ln -sf /usr/bin/python3.11 /usr/local/bin/python3.11

# Copy the application from the builder
COPY --from=builder --chown=app:app /app /app
WORKDIR /app

# Place executables in the environment at the front of the path
ENV PATH="/app/.venv/bin:$PATH"

# Run the service using the path to the handler
ENTRYPOINT python -u $HANDLER
```

We'll add a `dockerignore` to help reduce the size of the Docker Image that is being deployed.

```text title:transcribe.dockerignore
.mypy_cache/
.nitric/
.venv/
nitric-spec.json
nitric.yaml
README.md
```

And add `./model` to the python docker ignore.

```text tile:python.dockerignore
.mypy_cache/
.nitric/
.venv/
.model/
nitric-spec.json
nitric.yaml
README.md
```

Finally, we can update the project file to point our batch job to our new dockerfile.

```yaml title:nitric.yaml
name: podcast-transcription
services:
  - match: services/api.py
    runtime: python
    start: uv run watchmedo auto-restart -p *.py --no-restart-on-command-exit -R python -- -u $SERVICE_PATH

batch-services:
  - match: batches/transcribe.py
    runtime: transcribe
    start: uv run watchmedo auto-restart -p *.py --no-restart-on-command-exit -R python -- -u $SERVICE_PATH

runtimes:
  python:
    dockerfile: python.dockerfile
  transcribe:
    dockerfile: transcribe.dockerfile

preview:
  - batch-services
```

### Requesting a G instance quota increase

Most AWS accounts **will not** have access to on-demand GPU instances (G
Instances), if you'd like to run models using a GPU you'll need to request a quota increase for G instances.

If you prefer not to use a GPU you can set `gpus=0` in the `@transcribe_podcast` decorator in `batches/transcribe.py`.

<Note>
  **Important:** If the gpus value in `batches/transcribe.py` exceeds the number
  of available GPUs in your AWS account, the job will never start. If you want
  to run without a GPU, make sure to set `gpus=0` in the `@transcribe_podcast`
  decorator. This is just a quirk of how AWS Batch works.
</Note>

If you want to use a GPU you'll need to request a quota increase for G instances in AWS.

To request a quota increase for G instances in AWS you can follow these steps:

1. Go to the [AWS Service Quotas for EC2](https://console.aws.amazon.com/servicequotas/home/services/ec2/quotas) page.
2. Find/Search for **All G and VT Spot Instance Requests**
3. Click **Request quota increase**
4. Choose an appropriate value, e.g. 4, 8 or 16 depending on your needs

<img
  src="/docs/images/guides/ai-podcast/part-1/g-instance-quota-increase.png"
  style={{ maxWidth: 500, width: '100%', border: '1px solid #e5e7eb' }}
  alt="screen shot of requesting a G instance quota increase on AWS"
/>

Once you've requested the quota increase it may take time for AWS to approve it.

### Deploy the project

Once the above is complete, we can deploy the project to the cloud using:

```bash
nitric up
```

<Note>
  The initial deployment may take time due to the size of the python/Nvidia
  driver and CUDA runtime dependencies.
</Note>

Once the project is deployed you can try out some transcriptions, just add a podcast to the bucket and the bucket notification will be triggered.

<Note>
Running the project in the cloud will incur costs. Make sure to monitor your usage and shut down the project if you're done with it.

Running on g5.xlarge from testing this project will cost ~$0.05/minute of transcription. Based on standard EC2 pricing for US regions.

</Note>

You can destroy the project once it is finished using `nitric down`.

## Summary

In this guide, we've created a podcast transcription service using OpenAI Whisper and Nitric's Python SDK. We showed how to use batch jobs to run long-running workloads and connect these jobs to buckets to store generated transcripts. We also demonstrated how to expose buckets using simple CRUD routes on a cloud API. Finally, we were able to create dockerfiles with GPU support to optimize the generation speeds on the cloud.

For more information and advanced usage, refer to the [Nitric documentation](/).
